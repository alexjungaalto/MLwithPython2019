{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beaf6c42bb82b8aacd00e310dce77b02",
     "grade": false,
     "grade_id": "cell-a5592b3f1c6bf91d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#  Machine Learning with Python - Classification\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this exercise, you will learn how to formulate and solve a classification problem. A classification problem amounts to finding a good classifier which maps a given data point via its features to a particular label. The label indicates to which class or category the data point belongs. \n",
    "We will implement **logistic regression for binary classification** and use **gradient descent** to find the optimal classifier. We will also consider a simple approach to extend binary classifiers to multiclass problems which involve more than two categories.\n",
    "\n",
    "## Exercise Contents\n",
    "\n",
    "1. [Introduction](#1-Introduction) - Here we formulate a classification problem.\n",
    "2. [Data](#2-Data) - A description of the dataset.\n",
    "3. [Exercise](#3-Exercise) - A total of 5 tasks. Read the task descriptions and answer accordingly.\n",
    "    * 3.1 [**Getting Hands on the Data**](#3.1-Getting-Hands-on-the-Data)\n",
    "    * 3.2 [**Logistic Regression**](#3.2-Logistic-Regression)\n",
    "    * 3.3 [**Gradient Descent Step Size**](#3.3-Gradient-Descent-Step-Size)\n",
    "    * 3.4 [**Accuracy - How well did we do?**](#3.4-Accuracy---How-well-did-we-do?)\n",
    "    * 3.5 [**Multiclass Classification (One vs All)**](#3.5-Multiclass-Classification)\n",
    "\n",
    "## Keywords\n",
    "\n",
    "`Classification`,`Logistic Regression`, `Sigmoid Function`, `Gradient Descent (GD)`\n",
    "\n",
    "## Relevant Sections in [Course Book](https://arxiv.org/abs/1805.05052)  \n",
    "\n",
    "Section 2; Section 3.4\n",
    "\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Suppose you are an intern at the (fictive) company `Hunda` whose brand-new lawn mower robot uses its on-board camera to find out which surface it is currently moving on. Your job is to develop a firmware module which allows the mower robot to classify images generated by the camera according to the categories \"grass\", \"soil\", \"tiles\". To do so we will use a specific classification method named logistic regression. We will first apply a logistic regression to distinguish between \"grass\" and \"no grass\". Then we will use another application of logistic regression to distinguish non-grass images further between soil and tiles images. In order to develop image classifier you are provided with a bunch of snapshots that have been labeled by the previous summer intern. Thus, we can use this labeled data to train the image classifier. \n",
    "\n",
    "![](./images/banner.jpg)\n",
    "\n",
    "## 2 Data\n",
    "\n",
    "The dataset consists of $m=55$ images, stored in the folder named `images`:\n",
    "\n",
    "* 20 images of grass (stored in the files `image_1.jpg` to `image_20.jpg`)\n",
    "* 20 images of soil (stored in the files `image_21.jpg` to `image_40.jpg`)\n",
    "* 15 images of tiles (stored in the files `image_41.jpg` to `image_55.jpg`)\n",
    "\n",
    "You can use the Python package `PIL` (=the Python Imaging Library) to determine typical image characteristics (size, color etc). Some basic functions of PIL are demonstrated below, so you can use those functions in the exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a11a55de8d6e4bcecd4cc0ae3d81f682",
     "grade": false,
     "grade_id": "cell-245dfc59a01e2bab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import most of the required libraries for this exercise\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These imports are for testing purposes only\n",
    "\n",
    "from unittest.mock  import patch\n",
    "from plotchecker import ScatterPlotChecker\n",
    "\n",
    "# Read in an image from a jpg-file and store it in the variable \"im\"\n",
    "\n",
    "im = Image.open(\"images/image_1.jpg\");\n",
    "\n",
    "# Determine the width and height of the image\n",
    "\n",
    "width, height = im.size;\n",
    "print('width: %d, height: %d' % (width, height))\n",
    "\n",
    "# Convert the image to RGB bitmap\n",
    "\n",
    "rgb_im = im.convert('RGB');\n",
    "\n",
    "# Determine RGB values of the pixel at location (2,3) \n",
    "\n",
    "pixel = rgb_im.getpixel((2,3))\n",
    "print('Pixel (R, G, B): (%d, %d, %d)' % (pixel[0], pixel[1], pixel[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a74617978be08fdf0027a16873edf75",
     "grade": false,
     "grade_id": "cell-4a084869afc0c5ec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3 Exercise\n",
    "\n",
    "The actual exercise starts here and is divided into 5 part:\n",
    "\n",
    "* 3.1 [**Getting Hands on the Data**](#3.1-Getting-Hands-on-the-Data)\n",
    "* 3.2 [**Logistic Regression**](#3.2-Logistic-Regression)\n",
    "* 3.3 [**Gradient Descent Step Size**](#3.3-Gradient-Descent-Step-Size)\n",
    "* 3.4 [**Accuracy - How well did we do?**](#3.4-Accuracy---How-well-did-we-do?)\n",
    "* 3.5 [**Multiclass Classification**](#3.5-Multiclass-Classification)\n",
    "\n",
    "Your task is to fill in `...` under `### STUDENT TASK ###` in each step.\n",
    "\n",
    "## 3.1 Getting Hands on the Data\n",
    "\n",
    "Although the images are quite small (around $3000 \\times 3000$ pixels) we cannot easily process an image by just stacking the pixels into a vector since it would result in long vectors of length around $3000^2=9\\cdot 10^6$. The processing of such long vectors is challenging both, computationally as it requires a lot computation time and statistically since the resulting method is likely to overfit the training data (see Section 7 of the course book). Therefore, we will represent each image by only $n=3$ features which are given by the average red, green and blue components (\"redness\", \"greenness\" and \"blueness\") denoted $x_{r}$, $x_{g}$ and $x_{b}$, respectively. \n",
    "\n",
    "In what follows, we represent the $i$th image in the dataset using the feature vector $\\mathbf{x}^{(i)} = \\big(x_{r}^{(i)},x_{g}^{(i)},x_{b}^{(i)} \\big)^{T} \\in \\mathbb{R}^{3}$. The redness of the $i$th image is defined as \n",
    "\n",
    "\\begin{equation*}\n",
    "x_{r}  = (1/J) \\sum_{j=1}^{J} r^{(i)}_{j}\n",
    "\\end{equation*}\n",
    "\n",
    "where $r^{(i)}_{j}$ denotes the redness (on scale $0,\\ldots,255$) of the $j$th pixel in the $i$th image. The total number of pixels is denoted $J$. In particular, $x^{(1)}_{r}$ denotes the average red component of the first image in the dataset. The greenness $x_{\\rm g}$ and blueness $x_{\\rm b}$ are defined similarly. \n",
    "\n",
    "It will be convenient to stack the feature vectors $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{3}$, for $i=1,\\dots,m$, obtained for all images in the dataset into the feature matrix \n",
    "\n",
    "<a id='xm'></a>\n",
    "\\begin{equation*}\n",
    "    \\mathbf{X} = \\big(\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(55)}\\big)^T=\\begin{bmatrix}\n",
    "    x^{(1)}_{r}  & x^{(1)}_{g}  & x^{(1)}_{b} \\\\\n",
    "    \\vdots & \\ddots & \\vdots\\\\\n",
    "    x^{(55)}_{r} & x^{(55)}_{g} & x^{(55)}_{b}\n",
    "    \\end{bmatrix},\\ \\mathbf{X} \\in \\mathbb{R}^{m \\times n},\\ \\text{where } m=55, n=3.\n",
    "    \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "Beside its features $\\mathbf{x}^{(i)}$ the $i$th image in our dataset is characterized by the label $y^{(i)}$ which is $y^{(i)}=1$ if the image shows grass and $y^{(i)}=0$ otherwise (i.e. it shows either soil or tiles). It is notationally convenient, to collect the labels of all images in our dataset into the label vector \n",
    "\n",
    "<a id='vy'></a>\n",
    "\\begin{equation*}\n",
    "    \\mathbf{y}=\\big(y^{(1)},y^{(2)},\\ldots,y^{(m)} \\big)^{T} = \\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots\\\\\n",
    "    y^{(m)}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{m}.\n",
    "    \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "### Student Tasks:\n",
    "\n",
    "- 3.1.1: [Feature Matrix](#featurefunction)\n",
    "- 3.1.2: [Label Vector](#labelfunction)\n",
    "- 3.1.3: [Visualize Data](#visualizedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7f9cc284294cf2f6d34f687470af071",
     "grade": false,
     "grade_id": "cell-17287368c834f17a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='featurefunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Feature Matrix. \n",
    "\n",
    "- Implement a Python function `feature_matrix()` which returns the feature matrix ([1](#xm)) of size $55 \\times 3$.\n",
    "    - For each image, loop over all pixels of the image and to compute the average red $x_{\\rm r}$, green $x_{\\rm g}$ and blueness $x_{\\rm b}$ of the image. Stack the features of all images into the feature matrix ([1](#xm)).\n",
    "    - Remember to divide each R, G and B sum with the total pixel count $J$ (which might be different for different images) to get the average value for each image.\n",
    "    - The $i$th image corresponds to the $i$th row (containing $x^{(i)}_{\\rm r}$, $x^{(i)}_{\\rm g}$ and $x^{(i)}_{\\rm b}$) in the feature matrix. \n",
    "    - Most of the commands required for this task are in the [2.Data-section](#2-Data).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55bc72bc0a307a51f149e5bd2530114f",
     "grade": false,
     "grade_id": "cell-3d854013ba9d2483",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feature_matrix(m = 55):\n",
    "    \"\"\"\n",
    "    Generate a feature matrix representing the images in our dataset.\n",
    "    \n",
    "    :param m: scalar-like, type=int, number of images, default m=55. \n",
    "    \n",
    "    :return: array-like, shape=(m, n), feature-matrix with n=3 features. One feature for each color and each image.\n",
    "    \"\"\"\n",
    "    #initialize the feature matrix with zeros. \n",
    "    X = np.zeros((m,3))\n",
    "\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f6504ddb5e8abad84c3511c2452d119",
     "grade": true,
     "grade_id": "cell-840700cb14517bbf",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_matrix = feature_matrix(1)\n",
    "assert test_matrix.shape == (1,3), f'feature_matrix returns wrong matrix for m=1. It should be shape (1,3), but you gave {test_matrix.shape}'\n",
    "test_matrix = feature_matrix(2)\n",
    "assert test_matrix.shape == (2,3), f'feature_matrix returns wrong matrix for m=1. It should be shape (2,3), but you gave {test_matrix.shape}'\n",
    "np.testing.assert_allclose(test_matrix[0], [137, 164, 76],atol=1, err_msg='This is close, but not exact check that you are doing the correct sum average calculation.')\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a05830d34873b28d72c68b9e4a4a5b89",
     "grade": false,
     "grade_id": "cell-acac139fee4f522f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='labelfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Label Vector. \n",
    "\n",
    "- Implement a Python function `labels()` which returns the label vector ([2](#vy)).\n",
    "    - The vector should contain the **label** $y^{(i)}$ for each image. The label is $y^{(i)}=1$ if the $i$th image shows grass, $y^{(i)}=0$ otherwise.\n",
    "    - Hint: See the [2.Data-section](#2-Data).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3459a68aa8c13ec7c35ae007378ccd0f",
     "grade": false,
     "grade_id": "cell-b67c7f64aa86fbba",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def labels(m=55):\n",
    "    \"\"\"\n",
    "    Generate the label vector, where 1 is a Grass image and 0 is Non-Grass.\n",
    "    \n",
    "    :param m: scalar-like, type=int, amount of pictures\n",
    "    \n",
    "    :return: array-like, shape=(m, 1), label-vector\n",
    "    \"\"\"\n",
    "    y = np.zeros((m,1));\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d773c9f64820ccc7844076b4ed36dab2",
     "grade": true,
     "grade_id": "cell-1230c58d375ceb1a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_labels = labels()\n",
    "assert test_labels.shape == (55,1), f'Your label vector is incorrect shape. It should be (55,1), but you gave {test_labels.shape}'\n",
    "for i in [1,3,6,19]:\n",
    "    assert test_labels[i] == 1, f'image_{i+1}.jpg should be a grass picture, but you labeled it as non-grass'\n",
    "for i in [20,25,40,49,54]:\n",
    "    assert test_labels[i] == 0, f'image_{i+1}.jpg should be a non-grass picture, but you labeled it as grass'\n",
    "\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dbf7d2368f10aa195ee99284d80f611",
     "grade": false,
     "grade_id": "cell-670d5fe0ba5c9b40",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='visualizedata'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Data Visualization. \n",
    "\n",
    "Scatter plots can be helpful in order to reveal relations between the features and labels of data points.\n",
    "\n",
    "- Implement a Python function `Visualize_data(X,y)` which uses as input the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ (see ([1](#xm))) and the label vector $\\mathbf{y}=\\big(y^{(1)},\\ldots,y^{(m)}\\big)^{T}$ (see ([2](#vy))) and generates three **scatter plots**.\n",
    "    - One scatter plot for each color combination:\n",
    "        - greenness $x_{\\rm g}$ vs. redness $x_{\\rm r}$\n",
    "        - greenness $x_{\\rm g}$ vs. blueness $x_{\\rm b}$ \n",
    "        - redness $x_{\\rm r}$ vs. blueness $x_{\\rm b}$\n",
    "    - In these plots, mark each data point with a cross (\"x\") if it represents a grass image ($y^{(i)}=1$) and with a dot (\"$\\cdot$\") otherwise.\n",
    "    \n",
    "- The style of the plots are preconfigured. Your task is to use the correct input parameters (correct values for coordinates of data points) for the `plt.scatter()` function.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeb2a4a6d0a3acef3a3e26b487810115",
     "grade": false,
     "grade_id": "cell-9fe3bb2e6161169f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Visualize_data(X,y):\n",
    "    \"\"\"\n",
    "    Fill the blanks to generate correct visual demonstration of features\n",
    "\n",
    "    :param X: array-like, shape=(m, n), feature matrix where m is the amount of features\n",
    "    :param y: array-like, shape=(m, 1), label-vector\n",
    "    \n",
    "    :return: plot-axes, python plot library axes which includes all 3 plots\n",
    "    \"\"\"\n",
    "    indx_1 = np.where(y == 1)[0] # index of each grass picture.\n",
    "    indx_2 = np.where(y == 0)[0] # index of each non-grass picture.\n",
    "    \n",
    "    # Set figure size (width, height)\n",
    "    fig, axes = plt.subplots(1, 3,figsize=(15, 5))\n",
    "\n",
    "    '''\n",
    "    PLOT GREENNESS AGAINST REDNESS\n",
    "    - Make a scatterplot of the average greenness (x-axis) vs redness (y-axis). \n",
    "    - Indicate Grass images by a cross, and others by a dot.\n",
    "    '''\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[0].scatter(...,..., c='g', marker ='x', label='Grass')\n",
    "    #axes[0].scatter(...,..., c='r', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[0].set_xlabel('Greenness of Images')\n",
    "    axes[0].set_ylabel('Redness of Images')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 1.}$Green vs Red')\n",
    "\n",
    "    '''\n",
    "    PLOT GREENNESS AGAINST BLUENESS\n",
    "    - The same as above but now greenness (x-axis) vs blueness (y-axis).\n",
    "    '''\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[1].scatter(..., ..., c='g', marker ='x', label='Grass')\n",
    "    #axes[1].scatter(..., ..., c='b', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[1].set_xlabel('Greenness of Images')\n",
    "    axes[1].set_ylabel('Blueness of Images')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 2.}$Green vs Blue')\n",
    "\n",
    "    '''\n",
    "    PLOT REDNESS AGAINST BLUENESS\n",
    "    - The same as above but now redness (y-axis) vs blueness (x-axis).\n",
    "    '''\n",
    "    ### STUDENT TASK ###\n",
    "    #axes[2].scatter(..., ..., c='r', marker ='x', label='Grass')\n",
    "    #axes[2].scatter(..., ..., c='b', marker ='o', label='Soil+Tiles')\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    axes[2].set_xlabel('Redness of Images')\n",
    "    axes[2].set_ylabel('Blueness of Images')\n",
    "    axes[2].legend()\n",
    "    axes[2].set_title(r'$\\bf{Figure\\ 3.}$Red vs Blue')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f2bc8118df48afa40bfa24b718ade71",
     "grade": true,
     "grade_id": "cell-058cfcc1206a7c5b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y = labels()\n",
    "X = feature_matrix()\n",
    "\n",
    "# Full Vector\n",
    "# Let's label : Grass = 1 , Soil = 0, Tiles = 0\n",
    "assert X.shape == (55,3), f'Expected feature matrix to be shape (55,3), but it was {X.shape}'\n",
    "axes = Visualize_data(X,y)\n",
    "for i in range(len(axes)):\n",
    "    pc = ScatterPlotChecker(axes[i])\n",
    "    color1 = pc.colors[0]\n",
    "    color2 = pc.colors[-1]\n",
    "    for c in range(len(pc.colors)):\n",
    "        if c < 20:\n",
    "            np.testing.assert_array_equal(pc.colors[c],color1,f\"In Figure {i+1}. You assigned image_{c+1}.jpg color incorrectly\")\n",
    "        else:\n",
    "            np.testing.assert_array_equal(pc.colors[c],color2,f\"In Figure {i+1}. You assigned image_{c+1}.jpg color incorrectly\")\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93a59feb89be73090e440e450c394226",
     "grade": false,
     "grade_id": "cell-1bc8b6272485193d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.2 Logistic Regression\n",
    "Our goal is to find out the label $y$ of an image, with $y=1$ if the image shows grass and $y=0$ otherwise. This classification of an image has to be based solely on its features $\\mathbf{x} = (x_{\\rm r},x_{\\rm g},x_{\\rm b})^{T}$ given by the image redness $x_{\\rm r}$, greenness $x_{\\rm g}$ and blueness $x_{\\rm b}$. Similar to linear regression, logistic regression applies a linear function of the form $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to predict the label $y$ based on the features $\\mathbf{x} = \\big(x_{r},x_{g},x_{b} \\big)^{T} \\in \\mathbb{R}^{3}$ of the image. \n",
    "\n",
    "Given a linear predictor $h^{(\\mathbf{w})}(\\mathbf{x})$, with some weight vector $\\mathbf{w} \\in \\mathbb{R}^{n}$, we classify an image (with feature vector $\\mathbf{x}$) as $\\hat{y} = 1$ if $h^{(\\mathbf{w})}(\\mathbf{x})=\\mathbf{x}^{T} \\mathbf{w} \\geq 0$ and $\\hat{y}=0$ otherwise. In order to measure the quality of a particular classifier $h^{(\\mathbf{w})}$ we use the **logistic loss** defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}\\big((\\mathbf{x},y),h^{(\\mathbf{w})}\\big) = -y\\log\\big(\\sigma(h^{(\\mathbf{w})}(\\mathbf{x}))\\big)-(1-y)\\log\\big(1-\\sigma(h^{(\\mathbf{w})}(\\mathbf{x}))\\big)\n",
    "    \\label{loss}\n",
    "    \\tag{3}\n",
    "\\end{equation*}\n",
    "with the sigmoid function,\n",
    "<a id='sigmoid'></a>\n",
    "\\begin{equation*}\n",
    "    \\sigma(z)= \\frac{1}{1+{\\rm exp}(-z)}.\n",
    "    \\label{sigmoid}\n",
    "    \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "**Note that the expression \\eqref{loss} for the logistic loss applies only if the classes are encoded as $y=1$ and $y=0$. If the two classes are encoded as $y=1$ and $y=-1$, we obtain a different formula for the logistic loss.** \n",
    "\n",
    "Since we have $m=55$ labeled images, each of them characterized by the features $\\mathbf{x}^{(i)}$ and the true label $y^{(i)}$, we can evaluate the logistic loss for all those images to obtain the empirical risk\n",
    "\n",
    "<a id='er'></a>\n",
    "\\begin{align}\n",
    "\\mathcal{E}(\\mathbf{w}) & = (1/m) \\sum_{i=1}^{m} \\mathcal{L}((\\mathbf{x}^{(i)},y^{(i)}),\\ h^{(\\mathbf{w})}) \\nonumber \\\\ \n",
    "&  = (1/m) \\sum_{i=1}^{m} -y^{(i)}\\log\\big(\\sigma(\\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)-(1-y^{(i)})\\log\\big(1-\\sigma(\\mathbf{w}^{T}\\mathbf{x}^{(i)})\\big)\n",
    "   \\label{erm}\n",
    "    \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "Note that the empirical risk $\\mathcal{E}( \\mathbf{w})$ is a differentiable convex function of the weight vector $\\mathbf{w}$. Therefore, we can use **gradient descent (GD)** to find the weight vector $\\mathbf{w}_{\\rm opt}$ which minimizes the empirical risk $\\mathcal{E}(\\mathbf{w})$. In particular, GD constructs a sequence of weight vectors $\\mathbf{w}^{(k)}$ by iterating (=repeating) the GD update\n",
    "\n",
    "<a id='gd'></a>\n",
    "\\begin{equation*}\n",
    "    \\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} − \\alpha\\nabla \\mathcal{E}(\\mathbf{w}^{(k)})\n",
    "    \\label{gd}\n",
    "    \\tag{6}\n",
    "\\end{equation*}\n",
    "\n",
    "With appropriate step size $\\alpha$, the iterate $\\mathbf{w}^{(k)}$ becomes increasingly accurate approximation of the optimal weight vector,  i.e. $\\mathbf{w}^{(k)}$ converges to $\\mathbf{w}_{\\rm opt}$,\n",
    "\\begin{equation*}\n",
    "    \\lim_{k \\rightarrow \\infty} \\mathbf{w}^{(k)} = \\mathbf{w}_{\\rm opt}\n",
    "\\end{equation*}\n",
    "\n",
    "Assume we run the GD updates for $k$ iterations which results in the weight vector $\\mathbf{w}^{(k)}$ and corresponding classifier map $h^{(\\mathbf{w}^{(k)})}(\\mathbf{x}) = \\big(\\mathbf{w}^{(k)} \\big)^{T} \\mathbf{x}$. Using this classifier map, we can compute the predicted label for a new image with features $\\mathbf{x}=\\big(x_{r},x_{g},x_{b}\\big)^{T}\\in \\mathbb{R}^{3}$ via simple thresholding\n",
    "\n",
    "<a id='classify'></a>\n",
    "\\begin{equation*} \n",
    "    \\hat{y} = \\begin{cases} \n",
    "        1 &\\text{if}\\ \\mathbf{x}^{T} \\mathbf{w}^{(k)} \\geq 0\\\\\n",
    "        0 &\\text{if}\\ \\mathbf{x}^{T} \\mathbf{w}^{(k)} < 0.\n",
    "    \\end{cases}\n",
    "    \\label{eq_classify}\n",
    "    \\tag{7}\n",
    "\\end{equation*}\n",
    "\n",
    "### Student Tasks\n",
    "- 3.2.1: [Sigmoid Function](#sigmoidfunction)\n",
    "- 3.2.2: [Empirical Risk](#erfunction)\n",
    "- 3.2.3: [The Gradient](#gradientfunction)\n",
    "- 3.2.4: [Gradient Descent](#gradientdescentfunction)\n",
    "- 3.2.5: [Classification](#predictfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ba11c30a8a1c4c319560c25d9fad757",
     "grade": false,
     "grade_id": "cell-e8f85332fb8f47c9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='sigmoidfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Sigmoid Function. \n",
    "\n",
    "Implement a Python function `sigmoid_function(X,w)` which\n",
    "- reads in the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times 3}$ and weight vector $\\mathbf{w} \\in \\mathbb{R}^{3}$.\n",
    "- The function should return a vector of length $m$ whose entries are the function values ([4](#sigmoid)) of the sigmoid function.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "924a45b9b88d5b35940b11e71bdd4d3d",
     "grade": false,
     "grade_id": "cell-19f1305d0212c5d0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_func(X,w):\n",
    "    \"\"\"\n",
    "    Create a function that calculates the sigmoid function for given input\n",
    "    \n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features and m is amount of pictures\n",
    "    :param w: array-like, shape=(1, n), weight vector with length of amount of features\n",
    "\n",
    "    :return: array-like, shape=(m, 1), sigmoid function values. Each in [0,1] interval.\n",
    "    \"\"\"\n",
    "    ### STUDENT TASK ###\n",
    "    # sigmoid = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfae809eac6d9f8ccb7d91253121b8d3",
     "grade": true,
     "grade_id": "cell-95d280446871f544",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are just simple tests to check that your function outputs\n",
    "in correct format and gives a correct result with simple input\n",
    "\"\"\"\n",
    "test_input = np.zeros((10,2))\n",
    "test_output = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "sig_res = sigmoid_func(test_input, np.array([1,1]))\n",
    "assert sig_res.shape == test_output.shape, f'Your sigmoid function outputs array in incorrect shape. It should be (m,1), but it was {sig_res.shape}'\n",
    "np.testing.assert_array_equal(sig_res, test_output)\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='erfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Empirical Risk. \n",
    "\n",
    "\n",
    "Implement a Python function `empirical_risk(X,y,w)` which\n",
    "- reads in the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times 3}$, the label vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ and the weight vector $\\mathbf{w} \\in \\mathbb{R}^{3}$ of a classifier $h^{(\\mathbf{w})}$.\n",
    "- The function should return a scalar number which is the empirical risk ([5](#er)). \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6c0238c8dd7566a4a4db57ea7b55dbd",
     "grade": false,
     "grade_id": "cell-b48b12b83992f3cf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def empirical_risk(X,y,w):\n",
    "    \"\"\"\n",
    "    Calculate the empricial risk of the logistic regression with current weight vector\n",
    "\n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features\n",
    "    :param y: array-like, shape=(m, 1), label-vector\n",
    "    :param w: array-like, shape=(1, n), weight vector size of a feature\n",
    "    \n",
    "    :return: scalar-like, type=Integer, loss or risk between the label and the prediction\n",
    "    \"\"\"\n",
    "    loss = float('inf')\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5231f2e025731f42c364be73e8fb322c",
     "grade": true,
     "grade_id": "cell-b4ec259032a0cccb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are just simple tests to check that your function outputs\n",
    "in correct format and gives a correct result with simple input\n",
    "\"\"\"\n",
    "test_input_X =  np.array([[1,0]])\n",
    "test_input_y = np.array([1])\n",
    "test_input_w =  np.array([1,0])\n",
    "test_output = 0.3132\n",
    "er_res = empirical_risk(test_input_X,test_input_y,test_input_w)\n",
    "np.testing.assert_almost_equal(er_res, test_output,  decimal=3,err_msg=\"Your empirical_risk function outputs incorrectly in a test case\")\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9bbe0aeaf1c4d874675e6610627e101",
     "grade": false,
     "grade_id": "cell-5962f68727d02984",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='gradientfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> The Gradient.\n",
    "\n",
    "\n",
    "In order to use gradient descent for finding a good weight vector $\\mathbf{w}$, we need to be able to compute the gradient of the function $\\mathcal{E}(\\mathbf{w})$. \n",
    "\n",
    "Implement a Python function `gradient(X,y,w)` which\n",
    "- reads in the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$, the label vector $\\mathbf{y} \\in \\mathbb{R}^{m}$ and a weight vector $\\mathbf{w} \\in \\mathbb{R}^{n}$.\n",
    "- The function should return the gradient of the empirical risk $\\mathcal{E}(\\mathbf{w})$ ([5](#er)) i.e. the partial derivate of the loss function respect to $\\mathbf{w}$.\n",
    "    - Calculating the gradient is actually pretty lengthy and non-trivial job. If you are struggling with this, feel free to look it up from the interwebs. We don't judge, but we expect you to understand the calculation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "476c791952e4f549d397dc7380ca95e9",
     "grade": false,
     "grade_id": "cell-42a036d9b0cba489",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient(X,y,w):\n",
    "    \"\"\"\n",
    "    Calculate a gradient\n",
    "\n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features\n",
    "    :param y: array-like, shape=(m, 1), label-vector\n",
    "    :param w: array-like, shape=(1, n), weigh vector size of a feature\n",
    "    \n",
    "    :return: array-like, shape=(m, 1)\n",
    "    \"\"\"\n",
    "    ### STUDENT TASK ###\n",
    "    # grad = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7586b558c6b318d00b3cc710bc2e4501",
     "grade": true,
     "grade_id": "cell-669c54825bdbfc94",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are just simple tests to check that your function outputs\n",
    "in correct format and gives a correct result with simple input\n",
    "\"\"\"\n",
    "test_input_X = np.ones((10,3))\n",
    "test_input_y = np.ones(10)\n",
    "test_input_w = np.zeros(3)\n",
    "test_output = np.array([-0.5,-0.5,-0.5])\n",
    "grad_res = gradient(test_input_X, test_input_y, test_input_w)\n",
    "assert grad_res.shape == test_output.shape == (3,), f'Your Gradient function should output output a result with same shape as a weight vector. Yours was {grad_res.shape}'\n",
    "np.testing.assert_array_equal(grad_res, test_output,'You probably have incorrect calculations in your gradient function since result was not the same with test inputs')\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d047999e1d24720baab868a6ca74544",
     "grade": false,
     "grade_id": "cell-3f58f0591436845b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='gradientdescentfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Gradient Descent. \n",
    "\n",
    "Now that we have all the necessary components to create the final Gradient Descent function.\n",
    "So, implement a Python function `gradient_descent(X,y,step_size, K)` which\n",
    "- reads in the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$, the label vector $\\mathbf{y} \\in \\mathbb{R}^{m}$, the GD step size `step_size` and the number `K` of GD steps.\n",
    "- The function should deliver the weight vector $\\mathbf{w}^{(K)}$ obtained after $K$ GD steps and a vector of length $K$ whose $k$th entry is the empirical risk $\\mathcal{E}(\\mathbf{w}^{(k)})$ achieved by the weight vector $\\mathbf{w}^{(k)}$ generated after $k$ GD steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30f5c0517530345b5d2c65b21372784d",
     "grade": false,
     "grade_id": "cell-a3b865f4be31ad9c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,step_size, K=3000):\n",
    "    \"\"\"\n",
    "    Gradient Descent with Logistic Regression\n",
    "    \n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features\n",
    "    :param y: array-like, shape=(m, 1), label-vector\n",
    "    :param step_size: scalar-like, type=int, defines step size of each iteration of gradient descent\n",
    "    :param K: scalar-like, type=int, how many steps we should take.\n",
    "              Defaults to 3000. You can change this for personal testing to make gradient finish faster\n",
    "    \n",
    "    :return er_list: array-like, shape=(K,), vector containing error after each step of gradient descent\n",
    "    :return w: array-like, shape=(1, n), final weight vector after K iterations.\n",
    "    \"\"\"\n",
    "    n = X.shape[1]\n",
    "    # Initialize w as 1xn array.\n",
    "    w = np.zeros((1,n))\n",
    "    er_list = np.zeros(K)\n",
    "    for i in range(K):\n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return er_list, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dac12b6c1ac990c0927da1e9409d1e7e",
     "grade": true,
     "grade_id": "cell-d9f1226b36897570",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are just simple tests to check that your function outputs\n",
    "in correct format and gives a correct result with simple input\n",
    "\"\"\"\n",
    "test_input_X = np.array([[1,0]])\n",
    "test_input_y = np.array([1])\n",
    "test_output = np.array([[0.44,0]])\n",
    "grad_res = gradient_descent(test_input_X,test_input_y, 0.1, 10)\n",
    "\n",
    "assert len(grad_res[0]) == 10, f'Ten iterations should lead into 10 empirical risk results. You had {len(grad_res[0])}'\n",
    "assert grad_res[1].shape == test_output.shape == (1,2), f'Weight vector should be shape (1,2), but you got {grad_res[1].shape}'\n",
    "np.testing.assert_allclose(grad_res[1], test_output,atol=0.05)\n",
    "assert grad_res[1][0][1] == 0.0, 'In this test case, second value should be exactly zero. You are probably doing something wrong'\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1342eed9e89fb414aa8627a542676515",
     "grade": false,
     "grade_id": "cell-d564979a25e4e4d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='predictfunction'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Classification.\n",
    "\n",
    "After we have computed (approximately) the optimal weight vector using GD, we need to apply it to data points for  predicting their labels (see ([7](#classify))). Your task is to implement a Python function `predict_output(X,w)` which\n",
    "- takes as input a feature matrix $\\mathbf{X}=\\big(\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(m)}\\big)^{T} \\in \\mathbb{R}^{m \\times n}$, containing feature vectors $\\mathbf{x}^{(i)}$ in its rows, and a weight vector $\\mathbf{w}$ (which might have been obtained from GD).\n",
    "- The function should return a vector $\\hat{\\mathbf{y}}=\\big(\\hat{y}^{(1)},\\ldots,\\hat{y}^{(m)}\\big)^{T}$ of length $m$ containing the predicted labels $\\mathbf{\\hat{y}^{(i)}}$ (according to Eq. ([7](#classify))).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11339737a9b67d8183391da920b32907",
     "grade": false,
     "grade_id": "cell-19a9ddf471820c27",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_output(X,w):\n",
    "    \"\"\"\n",
    "    Calculate the prediction with original feature matrix and final weight vector\n",
    "    \n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features\n",
    "    :param w: array-like, shape=(1, n), weigh vector size of a feature\n",
    "    \n",
    "    :return: array-like, shape=(m, 1), prediction with given weight vector and feature matrix.\n",
    "    \"\"\"\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0dcefc373ca9a7825a248a3871f9bdd",
     "grade": true,
     "grade_id": "cell-c6c047bce3f53827",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are just simple tests to check that your function outputs\n",
    "in correct format and gives a correct result with simple input.\n",
    "\"\"\"\n",
    "with patch(\"__main__.sigmoid_func\",side_effect=sigmoid_func) as mock:\n",
    "    y_predict_test = predict_output(np.array([[-1,1],[0,1],[-1,1],[1,1]]),np.array([[1,0]]))\n",
    "    assert y_predict_test.shape == (4,1) , f'Your predict should be shape (4,1), but it was {y_predict_test.shape}'\n",
    "    np.testing.assert_array_equal(y_predict_test, np.array([[0],[1],[0],[1]]))\n",
    "    assert mock.called, 'You should call sigmoid_func during predict_output execution'\n",
    "\n",
    "for i in np.linspace(-1,1,100,endpoint=False):\n",
    "    if i < 0:\n",
    "        np.testing.assert_array_equal(\n",
    "            predict_output(np.array([[i,1]]),np.array([[1,0]])), #Input shape (1,1) feature matrix and shape (1,2) weight vector\n",
    "            np.array([[0]]), #Result should be\n",
    "            \"Incorrect output. Check that you defined and interpreted sigmoid/prediction correctly.\")\n",
    "    else:\n",
    "        np.testing.assert_array_equal(\n",
    "            predict_output(np.array([[i,1]]),np.array([[1,0]])), #Input shape (1,1) feature matrix and shape (1,2) weight vector\n",
    "            np.array([[1]]), # Result should be\n",
    "            \"Incorrect output. Check that you defined and interpreted sigmoid/prediction correctly.\")\n",
    "\n",
    "print('All tests passed! Good job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a575e8b56ce5a064a93bb62b2ba83ea6",
     "grade": true,
     "grade_id": "cell-80bab771073c9131",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the final testing ground where we go through the execution chain with real inputs.\n",
    "This also checks that you called each function required for gradient descent.\n",
    "\"\"\"\n",
    "step_size = 1e-5\n",
    "num_iter = 3000\n",
    "e_list, w_opt = gradient_descent(X,y,step_size,num_iter)\n",
    "print('The optimal weight vector is:', w_opt)\n",
    "y_hat = predict_output(X,w_opt)\n",
    "\n",
    "assert np.sum(e_list)/num_iter < 40\n",
    "assert y_hat.shape == (55,1)\n",
    "\n",
    "# Test that each function is called\n",
    "with patch(\"__main__.sigmoid_func\",side_effect=sigmoid_func) as mock:\n",
    "    gradient_descent(np.array([[0,1]]),np.array([0]),step_size,200)\n",
    "    assert mock.called, 'You should call sigmoid_func function during the execution of gradient_descent()'\n",
    "with patch(\"__main__.gradient\",side_effect=gradient) as mock:\n",
    "    gradient_descent(np.array([[0,1]]),np.array([0]),step_size,200)\n",
    "    assert mock.called, 'You should call gradient function during the execution of gradient_descent()'\n",
    "with patch(\"__main__.empirical_risk\",side_effect=empirical_risk) as mock:\n",
    "    gradient_descent(np.array([[0,1]]),np.array([0]),step_size,200)\n",
    "    assert mock.called, 'You should call empirical_risk function during the execution of gradient_descent()'\n",
    "print('All test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2622e49002fa377827cf518a197906b",
     "grade": false,
     "grade_id": "cell-6570b18f56ded61e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.3 Gradient Descent Step Size\n",
    "\n",
    "The performance of GD depends crucially on the step size or \"learning rate\" $\\alpha$ (see ([6](#gd))). This task requires you to investigate how different choices for the step size influence the behavior of GD. \n",
    "<a id='stepsize'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Choosing Step-Size. \n",
    "\n",
    "Implement a Python function `visualize_error(X, y)` which\n",
    "- reads in the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$, the label vector $\\mathbf{y} \\in \\mathbb{R}^{m}$\n",
    "- The function should run GD using the values `step_sizes=[0.1,0.5,1,5,10,16]` for the step size/learning rate. For each step-size determine  the empirical risk $\\mathcal{E}(\\mathbf{w}^{(k)})$ obtained by the weight vector $\\mathbf{w}^{(k)}$ delivered by GD after $k$ iterations. \n",
    "    - The function should generate two plots, both containing the same curves but using different colours. In particular, the second plot should contain 5 curves coloured blue and 1 curve coloured red.       \n",
    "        - Each curve corresponds to one choice for the step-size, resulting in 6 curves. Each curve should be generated using the empirical risk $\\mathcal{E}(\\mathbf{w}^{(k)})$ as a function of the GD iteration $k$.\n",
    "    - Determine which of the step size values results in the fastest decrease of the empirical risk and mark the corresponding curve using colour red (i.e. by changing `best=None` to right step size. For example if you think that the best step size is 1, choose `best=1`). Now you should obtain a plot where one line is red and rest of them are blue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d89f3ed982a809529405193dbdc4254",
     "grade": false,
     "grade_id": "cell-e8deda7ea56f6261",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_error(X, y ):\n",
    "    \"\"\"\n",
    "    Generate 2 plots which visualize the error over each gradient descent step\n",
    "    \n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features\n",
    "    :param y: array-like, shape=(m, 1), label-vector\n",
    "   \n",
    "    :return axes: plot-axes, python plot library axes which include both plots\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # how many GD steps we should take.\n",
    "    # Defaults to 2000. You can change this for personal testing to make gradient finish faster\n",
    "    \n",
    "    num_iter = 2000\n",
    "    \n",
    "    #  here we store the best learning rate/step-size\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    # Change best=None into step size from the list that provides the fastest converge. e.g best=1\n",
    "    best = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    #  different values to be used for the GD step size \n",
    "    \n",
    "    step_sizes=[0.1,0.5,1,5,10,16]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2,figsize=(12, 4))\n",
    "    for step in step_sizes:\n",
    "        ### STUDENT TASK ###\n",
    "        # Plot Error against Step Size\n",
    "        # loss_list, _ = \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        n = len(loss_list) # Size of list remains the same.\n",
    "        x_axes = np.linspace(0,n,n,endpoint=False)\n",
    "        axes[0].plot(x_axes, loss_list, label=step)\n",
    "        \n",
    "        ### STUDENT TASK ###\n",
    "        # Plot Error against Step Size.\n",
    "        # Now mark the best converge in red. Use value from best as a correct step size.\n",
    "        if step == best:\n",
    "            axes[1].plot(x_axes, loss_list, label=step, color=\"red\")\n",
    "        else:\n",
    "            axes[1].plot(x_axes, loss_list, label=step, color=\"blue\")\n",
    "    axes[0].set_xlabel('Number of Iterations')\n",
    "    axes[0].set_ylabel('Loss Function')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 4.}$Converge of GD')\n",
    "    axes[1].set_xlabel('Number of Iterations')\n",
    "    axes[1].set_ylabel('Loss Function')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 5.}$Converge of GD')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return best, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5cd8e0d0d187f9cb629c2a382d0ec89f",
     "grade": true,
     "grade_id": "cell-76b0b20fe659e6c3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "res0_1, axes = visualize_error(X/255, y)\n",
    "from plotchecker import LinePlotChecker\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    pc = LinePlotChecker(axes[i])\n",
    "    pc.assert_num_lines(6)\n",
    "    \n",
    "assert res0_1 in [0.1,0.5,1,5,10,16], \"You should choose the best Converge line from the given list\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d09096d48ad37aa969d26d0d4a5c4df5",
     "grade": false,
     "grade_id": "cell-16b2b1e46281f69a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.4 Accuracy - How well did we do?\n",
    "In order to assess how well our model works, we calculate the accuracy achieved by the classifier $h^{(\\mathbf{w})}$ obtained from task 3.3. We do this by computing the fraction of correctly labeled images, where the true label $y^{(i)}$ is equal to the predicted label $\\hat{y}^{(i)}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\text{Accuracy} =\\dfrac{1}{m} \\sum_{i=1}^{m} \\mathcal{I}(\\hat{y}^{(i)} = y^{(i)})\n",
    "    \\label{acc}\n",
    "    \\tag{8}\n",
    "\\end{equation*}\n",
    "\n",
    "Here $\\mathcal{I}(\\hat{y}^{(i)} = y^{(i)})$ denoes the indicator function which is equal to one if the argument is a correct statement, i.e., if $\\hat{y}^{(i)} = y^{(i)}$, and equal zero otherwise. \n",
    "\n",
    "<a id='accuracy'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Compute Accuracy. \n",
    " \n",
    "Implement a Python function `calculate_accuracy(y,y_hat)` according to (Eq. \\ref{acc}) which\n",
    "- takes as inputs a vector $\\mathbf{y}=\\big(y^{(1)},\\ldots,y^{(m)}\\big)^{T}$ of true labels and another vector $\\mathbf{\\hat{y}}=\\big(\\hat{y}^{(1)},\\ldots,\\hat{y}^{(m)}\\big)^{T}$ containing predicted labels.\n",
    "- The function should return the accuracy (Eq. \\ref{acc}) (as percentage).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38fefaef96f04295eeb8419b70ab33bd",
     "grade": false,
     "grade_id": "cell-46b5d6918ae35a38",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of your prediction\n",
    "    \n",
    "    :param y: array-like, shape=(m, 1), correct label vector\n",
    "    :param y_hat: array-like, shape=(m, 1), label-vector prediction\n",
    "    \n",
    "    :return: scalar-like, percentual accuracy of your prediction\n",
    "    \"\"\"\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return accuracy\n",
    "print ('Accuracy of the result is: %f%%' % calculate_accuracy(y,y_hat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47952b65a93a39937eaf76488aceb86f",
     "grade": true,
     "grade_id": "cell-62e401362b014895",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(100,calculate_accuracy(np.array([0]),np.array([0])))\n",
    "np.testing.assert_equal(0,calculate_accuracy(np.array([1]),np.array([0])))\n",
    "np.testing.assert_equal(50,calculate_accuracy(np.array([1,0]),np.array([0,0])))\n",
    "np.testing.assert_equal(25,calculate_accuracy(np.array([1,1,0,0]),np.array([0,0,1,0])))\n",
    "\n",
    "test_acc = calculate_accuracy(y,y_hat)\n",
    "\n",
    "assert 70 < test_acc < 100, \"Your accuracy should be above 70%\"\n",
    "assert 75 < test_acc, \"Your accuracy was too weak\"\n",
    "assert test_acc < 92, \"Your accuracy was too good. You are probably not using correct methods.\"\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b2a859d35ff8f30d8e175a256745522",
     "grade": false,
     "grade_id": "cell-130eebe8963b8793",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.5 Multiclass Classification\n",
    "\n",
    "We will now extend logistic regression, which we used for binary classification above, to classify images according to the three categories \"grass\", \"soil\" or \"tiles\". So is all of our previous work on classifying images into \"grass\" vs. \"no grass\" for nothing? Nope! Adapting a binary classification method (using two different label values such as $y=1$ and $y=0$) to this multiclass task is straightforward using the **“one vs rest” technique**.\n",
    "\n",
    "The idea is quite simple: split the multiclass problem into three subproblems, each subproblem being one binary classification problem as in [`3.3 Logistic Regression`](#3.2-Logistic-Regression). More specifically, we can solve the problem of classifying images into three classes \"grass\", \"soil\" or \"tiles\" by instead solving three subproblems (which are binary classification problems!):\n",
    "\n",
    "1. subproblem: classify images into \"grass\" $(y=1)$ vs. \"no grass\"$(y=0)$ \n",
    "2. subproblem: classify images into \"soil\" $(y=1)$ vs. \"no soil\" $(y=0)$ \n",
    "3. subproblem: classify images into \"tiles\" $(y=1)$ vs. \"no tiles\" $(y=0)$\n",
    "\n",
    "We can re-use the work done in [`3.3 Logistic Regression`](#3.2-Logistic-Regression), to solve each of these subproblems. The first subproblem has already been solved in [Sec. 3.2](#3.2-Logistic-Regression). For the other two subproblems, we only need to modify the label vector ([2](#vy)).\n",
    "\n",
    "As an example, the first image (which shows grass) has the label $y^{(1)}=1$ in subproblem 1 but a different label $y^{(1)}=0$ in subproblems 2 and 3, since it is neither soil nor tile. For each subproblem we get a different optimal weight vector ($\\mathbf{w}^{(\\rm grass)}$, $\\mathbf{w}^{(\\rm soil)}$ or $\\mathbf{w}^{(\\rm tiles)}$) by solving the empirical risk minimization problem (5) using GD. \n",
    "\n",
    "### Example\n",
    "\n",
    "Assume we want to classify a new image. We generate a feature vector $\\mathbf{x}=(x_{r},x_{g},x_{b})^{T}$ and use our predictor three times, yielding the following prediction values: \n",
    "\n",
    "1. subproblem: $h^{(\\mathbf{w}^{(\\rm grass)})}(\\mathbf{x}) = 0.1$ (\"grass vs. no grass\")\n",
    "2. subproblem: $h^{(\\mathbf{w}^{(\\rm soil)})}(\\mathbf{x}) = 0.4$ (\"soil vs. no soil\") \n",
    "3. subproblem: $h^{(\\mathbf{w}^{(\\rm tiles)})}(\\mathbf{x}) = 0.8$ (\"tiles vs. no tiles\")\n",
    "\n",
    "From these results, we can see that the predictor $h^{(\\mathbf{w}^{(\\rm tiles)})}(x)$ for subproblem 3 (`tiles` vs. `no tiles`) yields the highest confidence. Hence, we classify this image as `tiles`. \n",
    "\n",
    "<img src=\"./images/MulticlassHunda.jpg\" style=\"width: 600px\"/>\n",
    "\n",
    "### Student Tasks\n",
    "- 3.5.1: [Multiclass Labels](#sublabels)\n",
    "- 3.5.2: [Multiclass Gradient](#multgrad)\n",
    "- 3.5.3: [Multiclass Predict](#multpredict)\n",
    "- 3.5.4: [Multiclass Accuracy](#multacc)\n",
    "- 3.5.5: [Confusion Matrix](#multvis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06097b0b02149fb38b6b32db3d65fffa",
     "grade": false,
     "grade_id": "cell-0fe57d0e83d15cf2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='sublabels'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Multiclass Labels. \n",
    "\n",
    "Label vector should change depending on the subproblem.\n",
    "    \n",
    "- Implement a Python function `sub_labels()` which takes the number of data points $m$ and the subproblem index $k$ as an input:\n",
    " - $k=0$ means subproblem 1, where we try to predict which pictures are grass\n",
    "     - i.e. Grass pictures are equal to `1` where as all soil and tiles pictures are `0`\n",
    " - $k=1$ means subproblem 2, where we try to predict which picture represents soil\n",
    "     - i.e. Soil pictures are equal to `1` where as all grass and tiles pictures are `0`\n",
    " - $k=2$ means subproblem 3, where we try to predict which picture represents tiles\n",
    "     - i.e. Tile pictures are equal to `1` where as all grass and soil pictures are `0`\n",
    " - function should return the label vector $\\mathbf{y}$ for that subproblem based on $k$. \n",
    " \n",
    " **NOTE:** You can use the order of pictures to define the label vector. Use [2. Data-section](#2-Data) for reference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8276ddf38a7c59ae94f370beb6290f5",
     "grade": false,
     "grade_id": "cell-3fa563fa2e03f78c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sub_labels(m=55,k=0):\n",
    "    \"\"\"\n",
    "    Generate label vector for subproblem k\n",
    "    \n",
    "    :param m: scalar-like, number of pictures\n",
    "    :param k: scalar-like, subproblem number indication\n",
    "    \n",
    "    :return: array-like, shape=(m,1)\n",
    "    \"\"\"\n",
    "    y = np.zeros((m,1));\n",
    "    ### STUDENT TASK ###\n",
    "    ## Generate the label vector which has value 1 for the pictures of the subproblem we are currently looking at (indicated by k) \n",
    "    ## and 0 for the other two subproblems. \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "850c92bd5d08c2fe3dd8069704438cdd",
     "grade": true,
     "grade_id": "cell-f9d2b56ac04a18f5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_sub_labels = sub_labels(m=55,k=0)\n",
    "assert test_sub_labels.shape == (55,1), f'Your label vector should be shape (55,1), but it was {test_sub_labels.shape}'\n",
    "assert test_sub_labels[0] == 1, f'In subproblem 1, grass pictures should be 1, you have assigned picture in index 0 to {test_sub_labels[0]}'\n",
    "assert test_sub_labels[54] == 0, f'In subproblem 1, tile pictures should be 0, you have assigned picture in index 54 to {test_sub_labels[54]}'\n",
    "test_sub_labels = sub_labels(m=55,k=1)\n",
    "assert test_sub_labels[0] == 0, f'In subproblem 2, grass pictures should be 0, you have assigned picture in index 0 to {test_sub_labels[0]}'\n",
    "assert test_sub_labels[54] == 0, f'In subproblem 2, grass pictures should be 0, you have assigned picture in index 54 to {test_sub_labels[54]}'\n",
    "test_sub_labels = sub_labels(m=55,k=2)\n",
    "assert test_sub_labels[0] == 0, f'In subproblem 3, grass pictures should be 1, you have assigned picture in index 0 to {test_sub_labels[0]}'\n",
    "assert test_sub_labels[54] == 1, f'In subproblem 3, grass pictures should be 0, you have assigned picture in index 54 to {test_sub_labels[54]}'\n",
    "\n",
    "test_sub_labels = sub_labels(25,2)\n",
    "assert test_sub_labels.shape == (25,1), f'Incorrect label shape, it should be (25,1), but got {test_sub_labels.shape}'\n",
    "test_sub_labels = sub_labels(20,2)\n",
    "assert test_sub_labels.shape == (20,1), f'Incorrect label shape, it should be (20,1), but got {test_sub_labels.shape}'\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7baae7b4feb75624357ca23d52b5354b",
     "grade": false,
     "grade_id": "cell-5ddaea14e91a1fee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='multgrad'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Multiclass Learning.\n",
    "    \n",
    "Implement a Python function `multiclass_gradient_descent(X, step_size = 1e-5, steps = 3000)` which\n",
    "- reads in the feature matrix $\\mathbf{X}$ (see ([1](#xm))), step size (`step_size` with default =1e-5) and the number of GD iterations (`steps` with default 3000).\n",
    "- This function implements GD for the three subproblems and then outputs a matrix $\\mathbf{W} = \\big( \\mathbf{w}^{(\\rm grass)},\\mathbf{w}^{(\\rm soil)},\\mathbf{w}^{(\\rm tiles)}\\big)$. One for each subproblem\n",
    "    - Use the `gradient_descent()` and `sigmoid_func()` of [section 3.2](#3.2-Logistic-Regression) to get the optimal weight vector\n",
    "    \n",
    "**NOTE:** You should be able to complete this part by writing a couple of lines of code\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40c2157b75744b4a23b866003c168e8f",
     "grade": false,
     "grade_id": "cell-e7c42272f13e485a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_gradient_descent(X, step_size = 1e-5, steps = 3000):\n",
    "    \"\"\"\n",
    "    :param X: array-like, shape=(m,n), feature matrix, m pictures, n features\n",
    "    :param step_size: scalar-like, the step size of the gradient descent\n",
    "    :param steps: scalar-like, how many steps does the gradient descent do.\n",
    "\n",
    "    :return: array-like, shape=(n,3), 3 weight vectors (columns). One for each subproblem. n is amount of features\n",
    "    \"\"\"\n",
    "    sub_weights = np.zeros((X.shape[1],3))\n",
    "    for i in range(0,3):\n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return sub_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a5a07a50f613c30578651a12450562d",
     "grade": true,
     "grade_id": "cell-b9230f5661f5e892",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_mult_X = np.array([[1,0]])\n",
    "test_mult_grad = multiclass_gradient_descent(test_mult_X)\n",
    "assert test_mult_grad.shape == (2,3), f'Feature matrix with 2 features and 3 subproblems should result a weight vector matrix with 3 vectors and legnth 2. i.e. shape (2,3), but you gave {test_mult_grad.shape}'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "090164456aa1fd5a60f486a7b9004a54",
     "grade": false,
     "grade_id": "cell-4a451ffae1906c06",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='multpredict'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Multiclass Classification. \n",
    "  \n",
    "Now that we have a matrix a weight vectors, it's time to create predictions!\n",
    "\n",
    "Implement Python function `multiclass_predict()` where you:\n",
    "1. Iterate over each weight vector ($\\mathbf{w}^{(\\rm grass)}$, $\\mathbf{w}^{(\\rm soil)}$ or $\\mathbf{w}^{(\\rm tiles)}$)\n",
    "2. Calculate a prediction using feature matrix and chosen weight vector\n",
    "3. Store this prediction into a result matrix\n",
    "4. After all predictions have been calculated, choose a prediction with highest confidence\n",
    "    * Hint: if you store results in order into a result matrix, generating $\\hat{y}$ can be done with single command: `np.argmax`\n",
    "5. Return our prediction, $\\hat{y}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2317fece603d60a99b609749bfbfbabb",
     "grade": false,
     "grade_id": "cell-283ea63614bb6180",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_predict(X, weight_vectors):\n",
    "    \"\"\"\n",
    "    Calculate a prediction for each subproblem and\n",
    "    choose which class (0=grass,1=soil or 2=tiles) each picture fits the best.\n",
    "    i.e. which one of the 3 predictions (column) has the highest confidence?\n",
    "    \n",
    "    :param X: array-like, shape=(m, n), feature matrix where n is the amount of features and m amount of pictures\n",
    "    :param weight_vectors: array-like, shape=(n, 3), weigth vectors for data with n features. One weight vectors per subproblem\n",
    "    :param result: array-like, shape=(55,3)\n",
    "    \n",
    "    :return: array-like, shape=(55,1), final prediction.\n",
    "    \"\"\"\n",
    "    results = np.zeros((X.shape[0],3))\n",
    "    for c in range(0,3):\n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "accc4b42a52c660d60a7dcc990e37c72",
     "grade": true,
     "grade_id": "cell-0ff44c931b711635",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_predict_X = np.array([[1,0]])\n",
    "test_predict_w = np.array([[1,0,0],[1,0,0]])\n",
    "test_mul_predict = multiclass_predict(test_predict_X, test_predict_w)\n",
    "assert test_mul_predict.shape == (1,),f'Prediction with single row in feature matrix should result only one result, but you gave {test_mul_predict.shape}'\n",
    "assert test_mul_predict == 0, f'Result should be 0, but you gave {test_mul_predict}'\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67594ceed98427d1eb0ce4d746bc4216",
     "grade": false,
     "grade_id": "cell-e645d66704698379",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='multacc'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Student Task.</b> Multiclass Accuracy.\n",
    "\n",
    "Let's use the function that we defined [previously](#3.4-Accuracy---How-well-did-we-do?) to see what is prediction's overall accuracy.\n",
    "- Use the function `calculate_accuracy` of section 3.4 to calculate the accuracy. \n",
    "- We only need to define correct labels for `y_mult` based on [2. Data-section](#2-Data)\n",
    "    - Output should be a label vector of shape (55,1) where each row is one of 3 different class numbers (0, 1 or 2 representing grass, soil and tiles respectively)\n",
    "        - i.e. label vector used in this function is **not** an output from `sub_labels()`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f970b661a132dd052c8f6e1fdae2d277",
     "grade": false,
     "grade_id": "cell-dc269a407ebdeb75",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def multiclass_accuracy(y_hat):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of your prediction\n",
    "    :param y_hat: array-like, shape=(m, 1), label-vector prediction\n",
    "    \n",
    "    :return: scalar-like, accuracy of your prediction\n",
    "    \"\"\"\n",
    "    ### STUDENT TASK ###\n",
    "    #y_mult = \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return calculate_accuracy(y_mult, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0e0a20b3c611fd10f111e2b4d9d3f3b",
     "grade": true,
     "grade_id": "cell-3dbcccaca59cbc70",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w_opts = multiclass_gradient_descent(X)\n",
    "m_y_hat = multiclass_predict(X, w_opts)\n",
    "m_acc = multiclass_accuracy(m_y_hat)\n",
    "\n",
    "print(f'Weight Vectors:{w_opts}\\nPrediction: {m_y_hat}')\n",
    "assert m_y_hat.shape == (55,), f\"Incorrect shapes, {m_y_hat.shape} instead of (55,)\"\n",
    "assert m_acc > 75, f\"You accuracy should be over 75%. Yours was {m_acc}\"\n",
    "assert m_acc < 87, \"Your accuracy was too good. You are probably using incorrect methods.\"\n",
    "\n",
    "from unittest.mock  import patch\n",
    "with patch(\"__main__.gradient_descent\",side_effect=gradient_descent) as mock:\n",
    "    multiclass_gradient_descent(X)\n",
    "    assert mock.called, \"Remember to reuse functions that you have already defined.\"\n",
    "with patch(\"__main__.sigmoid_func\",side_effect=sigmoid_func) as mock:\n",
    "    multiclass_gradient_descent(X)\n",
    "    assert mock.called, \"Remember to reuse functions that you have already defined.\"\n",
    "with patch(\"__main__.calculate_accuracy\",side_effect=calculate_accuracy) as mock:\n",
    "    multiclass_accuracy(m_y_hat)\n",
    "    assert mock.called, \"Remember to reuse functions that you have already defined.\"\n",
    "    \n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fefc8115718757e115d6495a0a85bd1",
     "grade": false,
     "grade_id": "cell-f534cf30ed1f22d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='multvis'></a>\n",
    "    <div class=\" alert alert-warning\">\n",
    "<b>Demo.</b> Confusion Matrix. \n",
    "\n",
    "Computing the accuracy, as the fraction of correctly classified images, is one way to check how well you did. However, in some cases the accuracy can be misleading, particularly for applications where the different classes occur with significantly different probabilities (\"imbalanced data\"). A more fine-grained assessment of a classification method is provided by the confusion matrix. \n",
    "\n",
    "- After executing the cell below, you should see a confusion matrix. Assuming that you have completed previous steps\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8354263573f57186ae9293112bbf4d4",
     "grade": false,
     "grade_id": "cell-9f80746678ab6baf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def visualize_cm(cm):\n",
    "    \"\"\"\n",
    "    Function visualizes a confusion matrix with and without normalization\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2,figsize=(12, 4))\n",
    "\n",
    "    im1 = axes[0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    fig.colorbar(im1, ax=axes[0])\n",
    "    classes = ['grass','soil','tiles']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    axes[0].set_xticks(tick_marks)\n",
    "    axes[0].set_xticklabels(classes,rotation=45)\n",
    "    axes[0].set_yticks(tick_marks)\n",
    "    axes[0].set_yticklabels(classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        axes[0].text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    axes[0].set_xlabel('Predicted label')\n",
    "    axes[0].set_ylabel('True label')\n",
    "    axes[0].set_title(r'$\\bf{Figure\\ 6.}$Without normalization')\n",
    "    \n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    im2 = axes[1].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    fig.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    axes[1].set_xticks(tick_marks)\n",
    "    axes[1].set_xticklabels(classes,rotation=45)\n",
    "    axes[1].set_yticks(tick_marks)\n",
    "    axes[1].set_yticklabels(classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        axes[1].text(j, i, format(cm[i, j], '.2f'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    axes[1].set_xlabel('Predicted label')\n",
    "    axes[1].set_ylabel('True label')\n",
    "    axes[1].set_title(r'$\\bf{Figure\\ 7.}$Normalized')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute confusion matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y, m_y_hat)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# display the confusion matrix using an intensity plot\n",
    "\n",
    "visualize_cm(cnf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
